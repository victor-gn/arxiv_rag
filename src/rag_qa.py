from langgraph.graph import StateGraph, START, END
from typing_extensions import TypedDict
from typing import List, Dict, Any, Optional
from src import embedder
from src.vectorstore import QdrantVectorStore
import os
import logging
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

load_dotenv()

# Initialize LLM model
llm = ChatOpenAI(model="gpt-4.1-mini", temperature=0)

# LLM prompt template
RAG_PROMPT = ChatPromptTemplate.from_template(
    """Você é um assistente especializado em artigos científicos.
    Use o contexto fornecido para responder à pergunta do usuário.
    Se a resposta não estiver no contexto, diga que não tem informações suficientes.
    Mantenha as respostas concisas e baseadas nos artigos fornecidos.
    Cite os IDs dos artigos do arXiv quando mencionar informações deles.

    Contexto: {context}
    
    Pergunta: {question}
    
    Responda em português:"""
)


class State(TypedDict):
    """Type definition for the state of the RAG pipeline.

    Attributes:
        query: The user's question.
        query_embedding: The embedding vector of the query.
        retrieved_chunks: List of relevant text chunks retrieved from the vector store.
        response: The final response generated by the LLM.
        status: Current status of the pipeline execution.
        error: Error message if any error occurs during execution.
    """

    query: str
    query_embedding: List[float]
    retrieved_chunks: List[Dict[str, Any]]
    response: Optional[str]
    status: str
    error: Optional[str]


def generate_query_embedding_node(state: State) -> State:
    """Generates embedding vector for the user's query.

    This node takes the user's query and converts it into an embedding vector
    using OpenAI's embedding model.

    Args:
        state: Current state of the pipeline containing the user's query.

    Returns:
        Updated state with the query embedding and status information.
        If an error occurs, returns state with error information.
    """
    try:
        # Generate query embedding
        query_embedding = embedder.get_openai_embeddings([state["query"]])[0]
        logger.info("Successfully generated query embedding")
        return {
            "query_embedding": query_embedding,
            "status": "embedding_ok",
            "error": None,
        }
    except Exception as e:
        logger.error(f"Error generating query embedding: {str(e)}")
        return {
            "status": "error",
            "error": f"Erro ao gerar embedding da query: {str(e)}",
        }


def retrieve_chunks_node(state: State) -> State:
    """Retrieves relevant text chunks from the vector store.

    This node searches the vector store for chunks that are semantically
    similar to the query embedding.

    Args:
        state: Current state containing the query embedding.

    Returns:
        Updated state with retrieved chunks and status information.
        If no chunks are found or an error occurs, returns state with error information.
    """

    qdrant_vector_store = QdrantVectorStore()

    try:
        # Search for similar chunks
        results = qdrant_vector_store.search_similar_chunks(
            state["query_embedding"],
            collection_name=os.getenv("QDRANT_COLLECTION", "arxiv_chunks"),
            limit=10,
        )

        if not results:
            logger.warning("No relevant chunks found for the query")
            return {
                "status": "error",
                "error": "Nenhum chunk relevante encontrado para a query.",
            }

        logger.info(f"Retrieved {len(results)} chunks successfully")
        return {"retrieved_chunks": results, "status": "retrieval_ok", "error": None}
    except Exception as e:
        logger.error(f"Error retrieving chunks: {str(e)}")
        return {"status": "error", "error": f"Erro na busca de chunks: {str(e)}"}


def generate_response_node(state: State) -> State:
    """Generates a response using the LLM based on retrieved chunks.

    This node combines the retrieved chunks into a context and uses the LLM
    to generate a response to the user's query.

    Args:
        state: Current state containing the query and retrieved chunks.

    Returns:
        Updated state with the generated response and status information.
        If an error occurs, returns state with error information.
    """
    try:
        # Prepare context with retrieved chunks
        context = "\n\n".join(
            [
                f"[Artigo: {chunk['arxiv_id']}]\n{chunk['text']}"
                for chunk in state["retrieved_chunks"]
            ]
        )

        # Generate response using LLM
        messages = RAG_PROMPT.format(context=context, question=state["query"])
        response = llm.invoke(messages)

        logger.info("Successfully generated response")
        return {"response": response.content, "status": "done", "error": None}
    except Exception as e:
        logger.error(f"Error generating response: {str(e)}")
        return {"status": "error", "error": f"Erro na geração da resposta: {str(e)}"}


# Build the graph
graph = StateGraph(State)

# Add nodes
graph.add_node("generate_query_embedding", generate_query_embedding_node)
graph.add_node("retrieve_chunks", retrieve_chunks_node)
graph.add_node("generate_response", generate_response_node)

# Add edges
graph.add_edge(START, "generate_query_embedding")
graph.add_edge("generate_query_embedding", "retrieve_chunks")
graph.add_edge("retrieve_chunks", "generate_response")
graph.add_edge("generate_response", END)

# Compile the graph
rag_qa_pipeline = graph.compile()


def stream_qa_updates(query: str):
    """Streams the execution of the RAG pipeline and logs its progress.

    This function executes the RAG pipeline step by step and logs the progress
    of each node, including retrieved chunks and generated responses.

    Args:
        query: The user's question to be answered.

    Returns:
        None. The function logs the progress and results of the pipeline execution.
    """
    state = {"query": query}
    logger.info(f"Starting Q&A pipeline with query: {query}")

    for update in rag_qa_pipeline.stream(state, stream_mode="updates"):
        node_name = list(update.keys())[0]
        node_state = update[node_name]

        logger.info(f"Processing node: {node_name}")
        logger.info(f"Status: {node_state.get('status', 'N/A')}")

        if node_state.get("error"):
            logger.error(f"Error in {node_name}: {node_state['error']}")
            return

        if node_name == "retrieve_chunks":
            chunks = node_state.get("retrieved_chunks", [])
            logger.info(f"Retrieved {len(chunks)} chunks")
            if chunks:
                logger.info("Found articles:")
                for chunk in chunks:
                    logger.info(f"- {chunk['arxiv_id']}")
        elif node_name == "generate_response":
            logger.info("Generated response:")
            logger.info(node_state.get("response", ""))


if __name__ == "__main__":
    import sys

    if len(sys.argv) == 2 and sys.argv[1] == "viz":
        logger.info(rag_qa_pipeline.get_graph().draw_mermaid())
        logger.info(
            "\nCopy the Mermaid code above and paste it at https://mermaid.live to visualize the graph!"
        )
    elif len(sys.argv) < 2:
        logger.error(
            "Usage: python -m src.rag_qa_graph <query>  or  python -m src.rag_qa_graph viz"
        )
    else:
        query = sys.argv[1]
        stream_qa_updates(query)
